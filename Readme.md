<!-- NeuralDrifter | AI Toolsmith & Multi-Modal Creative -->

<div align="center">

<img width="100%" src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=0,2,2,5,30&height=220&section=header&text=NeuralDrifter&fontSize=80&fontAlignY=35&animation=twinkling&fontColor=76B900&desc=ai%20toolsmith%20%E2%80%A2%20multi-modal%20inference%20%E2%80%A2%20ai%20music&descAlignY=55&descSize=18"/>

[![Typing SVG](https://readme-typing-svg.demolab.com?font=JetBrains+Mono&weight=600&size=22&duration=3000&pause=1000&color=00D9FF&center=true&vCenter=true&multiline=true&repeat=true&width=700&height=80&lines=%24+freedom-cli+--list-models+%3E+ready;%24+mcp-server+--status+%3E+tools_online)](https://git.io/typing-svg)

<br>

![AI Toolsmith](https://img.shields.io/badge/AI-Toolsmith-0D1117?style=for-the-badge&logo=openai&logoColor=76B900)
![Multi-GPU](https://img.shields.io/badge/Hardware-Multi_GPU-0D1117?style=for-the-badge&logo=nvidia&logoColor=76B900)
![MCP Servers](https://img.shields.io/badge/MCP-Servers-0D1117?style=for-the-badge&logo=gnubash&logoColor=00D9FF)
![AI Music](https://img.shields.io/badge/Passion-AI_Music-0D1117?style=for-the-badge&logo=musicbrainz&logoColor=FF3333)

</div>

---

## `$ cat /etc/neural_drifter/about`

```python
class NeuralDrifter:
    """AI toolsmith. Building the tools that AIs use — and the rigs that run them."""

    def __init__(self):
        self.handle      = "NeuralDrifter"
        self.role        = "AI Toolsmith & Multi-Modal Creative"
        self.rig         = {
            "cpu": "Intel Core Ultra 7",
            "ram": "128GB DDR5",
            "board": "ASUS ProArt Z890 WiFi",
            "gpus": ["NVIDIA RTX 5070 Ti", "NVIDIA RTX 2080 Ti", "Intel B660 x2 (24GB VRAM each)"],
        }
        self.builds      = [
            "MCP servers & custom toolkits for LLMs",
            "Custom inference backend for Intel B660 GPUs",
            "Multi-modal AI pipelines — image, music, video, text",
            "Freedom CLI — use any LLM from your terminal",
        ]
        self.passions    = ["AI-generated music", "Local inference", "Giving AIs better tools"]
        self.philosophy  = "Data sovereignty matters — but the work comes first."

    def status(self):
        return "Tools online. Models loaded. Music generating."

>>> NeuralDrifter().status()
'Tools online. Models loaded. Music generating.'
```

---

## `$ nvidia-smi --current-ops`

<div align="center">

```
 ╔═══════════════════════════════════════════════════════════════════════╗
 ║  GPU   DEVICE                         VRAM          STATUS           ║
 ╠═══════════════════════════════════════════════════════════════════════╣
 ║  0     NVIDIA RTX 5070 Ti             16G           [CUDA]           ║
 ║  1     NVIDIA RTX 2080 Ti             11G           [CUDA]           ║
 ║  2     Intel B660 Arc                 24G           [INFERENCE]      ║
 ║  3     Intel B660 Arc                 24G           [INFERENCE]      ║
 ╠═══════════════════════════════════════════════════════════════════════╣
 ║  PID   PROCESS                              VRAM    STATUS           ║
 ╠═══════════════════════════════════════════════════════════════════════╣
 ║  1337  mcp-server --tools active            1.2G    [RUN]            ║
 ║  1338  freedom-cli --model llama3           8.1G    [RUN]            ║
 ║  1339  music-gen --compose                  6.4G    [RUN]            ║
 ║  1340  img-pipeline --generate              4.8G    [RUN]            ║
 ║  1341  b660-backend --inference             12.0G   [RUN]            ║
 ╠═══════════════════════════════════════════════════════════════════════╣
 ║  Total VRAM: 75G  |  CPU: Ultra 7  |  RAM: 128GB  |  Board: Z890   ║
 ╚═══════════════════════════════════════════════════════════════════════╝
```

</div>

---

## `$ lsmod | grep tech_stack`

<div align="center">

### AI Tools & Frameworks

![MCP](https://img.shields.io/badge/MCP-Servers-76B900?style=for-the-badge&logo=openai&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![HuggingFace](https://img.shields.io/badge/HuggingFace-FFD21E?style=for-the-badge&logo=huggingface&logoColor=black)
![Ollama](https://img.shields.io/badge/Ollama-353535?style=for-the-badge&logo=ollama&logoColor=white)
![vLLM](https://img.shields.io/badge/vLLM-0066FF?style=for-the-badge&logo=v&logoColor=white)

### Compute & Hardware

![CUDA](https://img.shields.io/badge/CUDA-76B900?style=for-the-badge&logo=nvidia&logoColor=white)
![Intel](https://img.shields.io/badge/Intel-Arc_B660-0071C5?style=for-the-badge&logo=intel&logoColor=white)
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![FastAPI](https://img.shields.io/badge/FastAPI-009688?style=for-the-badge&logo=fastapi&logoColor=white)

### Infrastructure & OS

![Linux](https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&logo=linux&logoColor=black)
![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![Bash](https://img.shields.io/badge/Bash-4EAA25?style=for-the-badge&logo=gnubash&logoColor=white)
![Git](https://img.shields.io/badge/Git-F05032?style=for-the-badge&logo=git&logoColor=white)

</div>

---

## `$ ls -la ~/projects/`

<div align="center">

<table>
<tr>
<td width="50%" valign="top">

### `./ai-tools/`

| Project | Description |
|---|---|
| **Freedom CLI** | Use a wide variety of LLMs from a single CLI. Model-agnostic, flexible, yours. |
| **MCP Servers** | Custom Model Context Protocol servers — building the tools that AIs use. |
| **LLM Toolkits** | Custom toolkits and integrations that give LLMs better capabilities. |
| **B660 Inference Backend** | Custom backend for Intel B660 GPUs to improve local inference performance. |

</td>
<td width="50%" valign="top">

### `./creative-ai/`

| Project | Description |
|---|---|
| **AI Music Apps** | Multiple AI music applications in development (not yet public). |
| **Image Pipelines** | Local image generation and processing workflows across multi-GPU setups. |
| **Video AI** | Video generation and processing with local inference. |

</td>
</tr>
</table>

</div>

---

## `$ uptime --stats`

<div align="center">

<img width="49%" src="https://github-readme-stats.vercel.app/api?username=NeuralDrifter&show_icons=true&hide_border=true&bg_color=0D1117&title_color=76B900&icon_color=00D9FF&text_color=C9D1D9&ring_color=76B900"/>
<img width="49%" src="https://github-readme-streak-stats.herokuapp.com/?user=NeuralDrifter&hide_border=true&background=0D1117&stroke=21262D&ring=76B900&fire=00D9FF&currStreakLabel=76B900&sideLabels=C9D1D9&currStreakNum=C9D1D9&dates=555555"/>

<br><br>

<img width="40%" src="https://github-readme-stats.vercel.app/api/top-langs/?username=NeuralDrifter&layout=compact&hide_border=true&bg_color=0D1117&title_color=76B900&text_color=C9D1D9"/>

</div>

---

## `$ netstat --collab`

<div align="center">

**Open for signal on:**

`MCP Servers & AI Tooling` · `AI Music Generation` · `Multi-Modal AI Pipelines` · `Local Inference Optimization`

<br>

![Profile Views](https://komarev.com/ghpvc/?username=NeuralDrifter&color=76B900&style=for-the-badge&label=SIGNALS+INTERCEPTED)

</div>

---

<div align="center">

<img width="100%" src="https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=0,2,2,5,30&height=120&section=footer"/>

```
"Build the tools. Run the models. Make the music."
```

</div>
