# Hey, I'm NeuralDrifter

I build tools for AI — MCP servers, CLI interfaces, local inference setups, and whatever else makes working with language models less painful. Most of my work runs locally on a multi-GPU rig because I prefer keeping things on-device.

I'm also into AI music generation, which is where a lot of the creative experimentation happens.

## Projects

**[Prism Relay](https://github.com/NeuralDrifter/prism-relay)** — MCP server that lets you query and compare responses from Claude, Gemini, DeepSeek, and LM Studio side by side, right from your AI coding assistant. Python.

**[Freedom CLI](https://github.com/NeuralDrifter/Freedom-Cli)** — Terminal interface for running a variety of LLMs from one place. Model-agnostic, no lock-in. TypeScript.

**[Rag Narock](https://github.com/NeuralDrifter/rag-narock)** — Local RAG system for searching indexed technical books and documents. Everything stays on your machine. Python.

**[Tools For AI](https://github.com/NeuralDrifter/Tools_For_AI)** — Collection of utilities and integrations for working with language models. Python.

## What I work with

Python, TypeScript, Rust, Bash — whatever fits the job. PyTorch, HuggingFace, Ollama, vLLM for inference. FastAPI when I need a backend. Docker and Linux for everything else.

Running on a multi-GPU setup (RTX 5070 Ti, RTX 2080 Ti, 2x Intel Arc B660) with 128GB RAM, mostly pointed at local inference and generation workloads.

## Get in touch

If you're working on MCP tooling, local inference, or AI music — I'm open to collaborating.
